{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TinyMPC's documentation!","text":"<p> Get Started  </p> <p>TinyMPC is an open-source solver tailored for convex model-predictive control that delivers high speed computation with a small memory footprint. Implemented in C++ with minimal dependencies, TinyMPC is particularly suited for embedded control and robotics applications on resource-constrained platforms. TinyMPC can handle state and input bounds and second-order cone constraints. A Python interface is available to aid in generating code for embedded systems.</p> <p>\ud83c\udfc6 TinyMPC won the Best Paper Award in Automation and was a finalist for Best Conference Paper Award and Best Student Paper Award at IEEE ICRA 2024! Thank you to everyone who has used TinyMPC and provided feedback!</p> <p> ICRA Paper  Conic Code Gen  Overview Video  </p>"},{"location":"#robot-demonstrations","title":"Robot demonstrations","text":"<p>TinyMPC contributes to bridging the gap between computationally intensive convex model-predictive control and resource-constrained processing platforms. Integrating TinyMPC into computationally underpowered robots enables them to execute agile maneuvers and exhibit safe behavior.</p>"},{"location":"#dynamic-obstacle-avoidance","title":"Dynamic obstacle avoidance","text":"<p>TinyMPC runs fast enough to enable re-linearizing constraints at each time step, allowing it to reason about moving obstacles. On the left, a virtual sphere centered at the end of the stick is linearized into a new set of hyperplane constraints at each time step. The algorithm can additionally handle any number of arbitrary linear constraints. On the right, for example, it is avoiding the end of the stick while staying in the yz plane.</p>"},{"location":"#extreme-pose-recovery","title":"Extreme pose recovery","text":"<p>TinyMPC can enable recovering from extreme initial conditions. In this example, it is compared against three of the Crazyflie 2.1's stock controllers. Only TinyMPC was able to reason about the control limits, thereby exibiting a clean-cut recovering maneuver.</p>"},{"location":"#figure-8-tracking","title":"Figure-8 tracking","text":"<p>We compared against the same stock controllers for an infeasible fast figure-8 tracking task (the time given to complete a single figure-8 could only be met if the drone was much more powerful). TinyMPC and PID were able to stay upright, but TinyMPC's trajectory more closely resembled a figure-8.</p>"},{"location":"#microcontroller-benchmarks","title":"Microcontroller benchmarks","text":"<p>TinyMPC outperforms state-of-the-art solvers in terms of speed and memory footprint on microcontroller benchmarks. </p>          Here, we solve randomly generated QP-based MPC problems and compare iteration times and memory footprint against OSQP. TinyMPC exibits a maximum speed-up of 8x over OSQP with much less memory.                   TinyMPC is also capable of handling conic constraints. In (b), we benchmarked TinyMPC against two existing conic solvers with embedded support, SCS and ECOS, on the rocket soft-landing problem. TinyMPC achieves an average speed-up of 13x over SCS and 137x over ECOS.                   Real-time control requires a solver to return a solution within a strict time window. We compared TinyMPC's trajectory tracking performance against SCS and ECOS on the rocket soft-landing problem while artificially changing the amount of time available for each solve. TinyMPC violates constraints less and has lower tracking error than SCS and ECOS at all control durations."},{"location":"#made-by","title":"Made by","text":"Anoushka Alavilli                       Khai Nguyen                       Sam Schoedel                       Elakhya Nedumaran                       Prof. Brian Plancher                       Prof. Zac Manchester"},{"location":"#citing","title":"Citing","text":"<pre><code>@inproceedings{tinympc,\n      title={TinyMPC: Model-Predictive Control on Resource-Constrained Microcontrollers}, \n      author={Khai Nguyen and Sam Schoedel and Anoushka Alavilli and Brian Plancher and Zachary Manchester},\n      year={2024},\n      booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n}\n</code></pre> <pre><code>@misc{tinympc-conic-codegen,\n      title={Code Generation for Conic Model-Predictive Control on Microcontrollers with TinyMPC}, \n      author={Sam Schoedel and Khai Nguyen and Elakhya Nedumaran and Brian Plancher and Zachary Manchester},\n      year={2024},\n      eprint={2403.18149},\n      archivePrefix={arXiv},\n}\n</code></pre>"},{"location":"get-started/examples/","title":"Using TinyMPC","text":"<p>Here we show how to use TinyMPC to control a cart-pole and 3D quadrotor with the python interface. Make sure you have installed the package.</p> <p>Check out our GitHub repository for examples in C++, and visit our GitHub Discussions page for any questions related to the solver!</p>"},{"location":"get-started/examples/#set-up-problem","title":"Set up problem","text":"<p>TinyMPC requires four matrices (A, B, Q, and R) and one number (N) to use. A and B describe the linearized system dynamics and Q and R are the costs on the state and control inputs. N is the length of the prediction horizon (or the number of time steps in the problem). This page assumes you already have a discrete, linearized model of your system dynamics (A and B). The next page walks through obtaining these starting from a nonlinear model.</p> QuadrotorCart-pole <p>For the quadrotor, we use the linearized model of the discretized quadrotor dynamics to stabilize about a hovering position. The state is composed of twelve variables: the three dimensional position, orientation, translational velocity, and angular velocity, which looks like \\(x = [p_x, p_y, p_z, \\theta_x, \\theta_y, \\theta_z, v_x, v_y, v_z, \\omega_x, \\omega_y, \\omega_z]^T\\). The control input is a four dimensional vector describing the thrust of each motor, and looks like \\(u = [u_1, u_2, u_3, u_4]^T\\).</p> <p>In this case, because the dynamics were linearized about a state which required some nominal thrust \\(u_\\text{hover}\\) to keep the quadrotor airborne, the solution from TinyMPC is actually a \\(\\Delta u\\) from \\(u_\\text{hover}\\). In a real implementation, the command that should be sent to the quadrotor is the absolute thrust \\(\\Delta u + u_\\text{hover}\\), where \\(u_\\text{hover}\\) depends on the specific quadrotor. (1)</p> <ol> <li>In the case of the cart-pole, the output from TinyMPC was still a \\(\\Delta u\\). However, because we linearized about an equilibrium position that required no control input, this was a \\(\\Delta u\\) from zero, and so there was no difference between the computed \\(\\Delta u\\) and the absolute value \\(u\\) which would be sent to the cart-pole's motor controller.</li> </ol> <pre><code>import tinympc\nimport numpy as np\n\n# Define necessary data\nA = np.array([ # (1)\n    [1.0, 0.0, 0.0, 0.0, 0.0245250, 0.0, 0.050, 0.0, 0.0, 0.0, 0.02044, 0.0],\n    [0.0, 1.0, 0.0, -0.0245250, 0.0, 0.0, 0.0, 0.050, 0.0, -0.02044, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050, 0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0250000, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0250000, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025],\n    [0.0, 0.0, 0.0, 0.0, 0.9810000, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0122625, 0.0],\n    [0.0, 0.0, 0.0, -0.9810000, 0.0, 0.0, 0.0, 1.0, 0.0, -0.0122625, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n\nB = np.array([ # (2)\n    [-0.07069, 0.07773, 0.07091, -0.07795],\n    [0.07034, 0.07747, -0.07042, -0.07739],\n    [0.0052554, 0.0052554, 0.0052554, 0.0052554],\n    [-0.1720966, -0.1895213, 0.1722891, 0.1893288],\n    [-0.1729419, 0.1901740, 0.1734809, -0.1907131],\n    [0.0123423, -0.0045148, -0.0174024, 0.0095748],\n    [-0.0565520, 0.0621869, 0.0567283, -0.0623632],\n    [0.0562756, 0.0619735, -0.0563386, -0.0619105],\n    [0.2102143, 0.2102143, 0.2102143, 0.2102143],\n    [-13.7677303, -15.1617018, 13.7831318, 15.1463003],\n    [-13.8353509, 15.2139209, 13.8784751, -15.2570451],\n    [0.9873856, -0.3611820, -1.3921880, 0.7659845]])\n\nQ = np.diag([1.0, 1.0, 1.0, 0.4, 0.4, 0.4, # (3)\n            0.4, 0.4, 0.4, 0.2, 0.2, 0.2]);\nR = np.diag([1000.0]*4); # (4)\n\nN = 20 # (5)\n\n# Set up the problem\nprob = tinympc.TinyMPC()\nprob.setup(A, B, Q, R, N)\n\n# Define initial condition\nx0 = np.array([0.5, 1.3, -0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code></pre> <ol> <li>This is the state transition matrix, which you get when linearizing the discretized version of your model's full nonlinear dynamics (in this case the cart-pole dynamics, described on this page) with respect to the state.</li> <li>This is the input or control matrix, which you get when linearizing the discretized version of your model's full nonlinear dynamics (in this case the cart-pole dynamics, described on this page) with respect to the input.</li> <li>This is the stage cost for the state, and defines how much to penalize the state for deviating from the reference state at each time step in the horizon. Change this to modify the controller's behavior.</li> <li>This is the stage cost for the input, and defines how much to penalize the input for deviating from the reference control at each time step in the horizon. Change this to modify the controller's behavior.</li> <li>This is the length of the horizon, and can be anything greater than one. The problem size scales linearly with this variable.</li> </ol> <p>For the cart-pole, we use the linearized model of the discretized cart-pole dynamics to stabilize about the upright position. The state is the position of the cart, the angle of the pole, the velocity of the cart, and the angular velocity of the pole, which looks like \\(x = [p, \\theta, v, \\omega]^T\\). The control input is a single force \\(u\\) acting on the cart. (1)</p> <ol> <li>TinyMPC always produces a \\(\\Delta u\\) and \\(\\Delta x\\) about the linearization point. Because we linearized the cart-pole about an equilibrium position that required no control input, \\(\\Delta u\\) = \\(u\\). Additionally, as discussed in this page, because we defined the coordinate frame of our cart-pole system such that the vertical equilibrium position (which is where we linearized) corresponds to a state of all zeros, \\(\\Delta x\\) = \\(x\\). This is irrelevant for the following example, but is important to keep in mind when simulating the system with its full dynamics or applying a control input when the linearization point is not at \\(x=0\\) or \\(u=0\\).</li> </ol> <pre><code>import tinympc\nimport numpy as np\n\n# Define necessary data\nA = np.array([[1.0, 0.01, 0.0, 0.0], # (1)\n              [0.0, 1.0, 0.039, 0.0],\n              [0.0, 0.0, 1.002, 0.01],\n              [0.0, 0.0, 0.458, 1.002]])\nB = np.array([[0.0  ], # (2)\n              [0.02 ],\n              [0.0  ],\n              [0.067]])\nQ = np.diag([10.0, 1, 10, 1]) # (3)\nR = np.diag([1.0]) # (4)\n\nN = 20 # (5)\n\n# Set up the problem\nprob = tinympc.TinyMPC()\nprob.setup(A, B, Q, R, N)\n\n# Define initial condition\nx0 = np.array([0.5, 0, 0, 0])\n</code></pre> <ol> <li>This is the state transition matrix, which you get when linearizing the discretized version of your model's full nonlinear dynamics (in this case the cart-pole dynamics, described on this page) with respect to the state.</li> <li>This is the input or control matrix, which you get when linearizing the discretized version of your model's full nonlinear dynamics (in this case the cart-pole dynamics, described on this page) with respect to the input.</li> <li>This is the stage cost for the state, and defines how much to penalize the state for deviating from the reference state at each time step in the horizon. Change this to modify the controller's behavior.</li> <li>This is the stage cost for the input, and defines how much to penalize the input for deviating from the reference control at each time step in the horizon. Change this to modify the controller's behavior.</li> <li>This is the length of the horizon, and can be anything greater than one. The problem size scales linearly with this variable.</li> </ol>"},{"location":"get-started/examples/#solve-problem","title":"Solve problem","text":"<pre><code># Set the first state in the horizon\nprob.set_x0(x0)\n\n# Solve the problem\nsolution = prob.solve()\n\n# Print the controls at the first time step\nprint(solution[\"controls\"])\n</code></pre>"},{"location":"get-started/examples/#model-predictive-control","title":"Model-predictive control","text":"<p>To use TinyMPC as a controller, all we have to do is solve in a loop, setting the first state in the horizon to the most recent estimate of the system's state at each time step. This estimate can come from a Kalman filter, for example. In this simple example we assume we have a perfect estimate of the state. On an actual robot, the simulation step would just be the physics from the real world acting on the system. To more accurately predict what that would look like, you can simulate the nonlinear system dynamics forward by one time step using an implicit integrator, of which there are many. We simulate with the linearized system dynamics for brevity.</p>"},{"location":"get-started/examples/#solve","title":"Solve","text":"<pre><code># Simulate for an arbitrary number of time steps\nNsim = 350\nxs = np.zeros((Nsim, Q.shape[0])) # History of states for plotting\nus = np.zeros((Nsim, R.shape[0])) # History of controls for plotting\nfor i in range(Nsim):\n    prob.set_x0(x0) # Set the first state in the horizon\n    solution = prob.solve() # Solve the problem\n    x0 = A@x0 + B@solution[\"controls\"] # Simulate the system (1)\n    xs[i] = x0\n    us[i] = solution[\"controls\"]\n</code></pre> <ol> <li>In this rudimentary example we simulate the system forward in time with the linearized dynamics. However, for a legitimate simulation this should be done by querying the nonlinear, discretized system dynamics, which might be implemented using the cartpole_rk4 function shown here, for example.</li> </ol>"},{"location":"get-started/examples/#plot-solution","title":"Plot solution","text":"<p><code>pip install matplotlib</code> if not already installed.</p> QuadrotorCart-pole <pre><code>import matplotlib.pyplot as plt\n\n# Plot trajectory\nfig, axs = plt.subplots(2, 1, sharex=True)\naxs[0].plot(xs[:,:3], label=[\"x\", \"y\", \"z\"])\naxs[1].plot(us, label=[\"u1\", \"u2\", \"u3\", \"u4\"])\naxs[0].set_title(\"quadrotor trajectory over time\")\naxs[1].set_xlabel(\"time steps (100Hz)\")\naxs[0].legend()\naxs[1].legend()\nplt.show()\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\n# Plot trajectory\nfig, axs = plt.subplots(2, 1, sharex=True)\naxs[0].plot(xs, label=[\"x (meters)\", \"theta (radians)\", \"x_dot (m/s)\", \"theta_dot (rad/s)\"])\naxs[1].plot(us, label=\"control (Newtons)\")\naxs[0].set_title(\"cartpole trajectory over time\")\naxs[1].set_xlabel(\"time steps (100Hz)\")\naxs[0].legend()\naxs[1].legend()\nplt.show()\n</code></pre>"},{"location":"get-started/examples/#code-generation","title":"Code generation","text":"<p>Generating code looks very similar to what we just did. All we have to do is set up the problem like before and code can be generated into a specified directory. Source, CMake, and example main files are copied or generated in the new directory. The generated code is then compiled into a python module that can be imported and used to validate its behavior before integrating it with a system.</p> <p>The following code generates a solver for a cartpole with control bounds of -0.5 and 0.5 Newtons, and limits the maximum number of iterations at each time step to 50. It puts everything in a folder called \"generated_code\".</p> <pre><code>import tinympc\nimport numpy as np\n\nA = np.array([[1.0, 0.01, 0.0, 0.0],\n              [0.0, 1.0, 0.039, 0.0],\n              [0.0, 0.0, 1.002, 0.01],\n              [0.0, 0.0, 0.458, 1.002]])\nB = np.array([[0.0  ],\n              [0.02 ],\n              [0.0  ],\n              [0.067]])\nQ = np.diag([10.0, 1, 10, 1])\nR = np.diag([1.0])\n\nN = 20\n\nprob = tinympc.TinyMPC()\n\nu_min = np.array([-0.5])\nu_max = np.array([0.5])\nprob.setup(A, B, Q, R, N, max_iter=50, u_min=u_min, u_max=u_max)\n\nprob.codegen(\"generated_code\", verbose=1)\n</code></pre>"},{"location":"get-started/examples/#validate-with-python","title":"Validate with python","text":"<p>TinyMPC will automatically compile the generated code into a new python module called <code>tinympcgen</code>, which will exist inside the output folder as a file with the name <code>tinympcgen.python-version-and-system-type.so</code>. To use it, just <code>import tinympcgen</code>, then set the reference trajectory if desired with <code>set_x_ref</code> and <code>set_u_ref</code>, set the initial state with <code>set_x0</code>, and then call <code>solve</code>. The python interpreter must have access to the newly generated module. This can be done by creating new a python file with the following code in the generated directory, for example.</p> <pre><code>import numpy as np\nimport tinympcgen\nimport matplotlib.pyplot as plt\n\ntinympcgen.set_x_ref(np.array([1.0, 0, 0, 0])) # Set the goal position (1)\ntinympcgen.set_x0(np.array([0.5, 0, 0, 0])) # Set first state in horizon\nsolution = tinympcgen.solve()\nprint(solution[\"controls\"])\n</code></pre> <ol> <li>The reference trajectory can also be set for each time step in the horizon, which is what's normally done when tracking a reference trajectory instead of just stabilizing about a point. <code>set_x_ref</code> expects the entire trajectory reference to be a numpy array of the shape (nx x N) and <code>set_u_ref</code> expects a numpy array of the shape (nu x N-1). You can also just set the reference for a single time step as done here, and the python module will handle expanding it to the entire horizon.</li> </ol>"},{"location":"get-started/examples/#build-with-cmake","title":"Build with CMake","text":"<p>Finally, a single top-level CMake file is provided for building an example executable called <code>tiny_codegen_example</code>, which has its main function inside <code>tiny_main.cpp</code>. To build it, run</p> <pre><code>cd generated_code/build # (1)\ncmake ..\ncmake --build .\n</code></pre> <ol> <li>The build folder should have been generated when TinyMPC built the python module, but if for some reason it did not, run <code>mkdir build</code> once inside the generated_code directory.</li> </ol> <p>Run the example executable from the build directory</p> <pre><code>./tiny_codegen_example\n</code></pre> <p><code>tiny_main.cpp</code> only calls <code>tiny_solve</code>, but all of the convenience functions available in the <code>tiny_api.hpp</code> header can be used to set the reference trajectory and update the initial state. <code>tiny_main.cpp</code> should be used as a starting point for integrating TinyMPC with your own project.</p>"},{"location":"get-started/installation/","title":"Installing TinyMPC","text":"<p>A python interface is available that allows for direct usage of TinyMPC. The interface can also be used to generate C++ code and an associated python module which allows for quick testing before integrating the generated code with your project. We provide examples for a few robots and have firmware for running TinyMPC on the Crazyflie 2.1 quadrotor.</p> <p>Source code is here. Check out the Python interface repository for implementation details.</p> <p>Visit our GitHub Discussions page for any questions related to the solver!</p>"},{"location":"get-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<p>Currently only available on Linux operating systems</p> <pre><code>pip install --upgrade tinympc\n</code></pre> <p>Go to the examples page to see how to use TinyMPC.</p>"},{"location":"get-started/installation/#build-from-source","title":"Build from source","text":"<p>If you'd like to build from source, you can do so by following these steps: </p> <p>Clone the repository and build the project</p> <pre><code>git clone https://github.com/TinyMPC/TinyMPC.git\ncd TinyMPC\nmkdir build\ncd build\ncmake ..\ncmake --build .\n</code></pre> <p>Run an example from the build directory</p> <pre><code>./examples/quadrotor_hovering\n</code></pre>"},{"location":"get-started/model/","title":"Model","text":""},{"location":"get-started/model/#linearization","title":"Linearization","text":"<p>TinyMPC in its vanilla implementation can only handle linear dynamics, which means systems must be linearized about an equilibrium before being used by the solver. Extensions to TinyMPC allow the user to better approximate a system's nonlinear dynamics by storing multiple linearizations, but we will start here with only one.</p> <p>A discrete, linearized system takes the form \\(x_{k+1} = Ax_k + Bu_k\\), where \\(x_k\\) and \\(u_k\\) are the state and control at the current time step, \\(A\\) is the state-transition matrix, \\(B\\) is the control or input matrix, and \\(x_{k+1}\\) is the state at the next time step. Usually, this is derived from the continuous, nonlinear dynamics of the system, which takes the form \\(\\dot{x} = f(x, u)\\), where \\(f\\) describes the instantaneous change in state at the current state and control input.</p> <p>This page describes how to derive the discrete, linearized system dynamics from the continuous, nonlinear system dynamics.</p>"},{"location":"get-started/model/#cart-pole-example","title":"Cart-pole example","text":"<p>The continuous time dynamics for the cart-pole have been derived many times. For this example we'll use the convention from this derivation, where the pole is upright at \\(\\theta=0\\). If we ignore friction, the only equations we care about in that derivation are (23) and (24). (1)</p> <ol> <li>If you want to use a cart-pole model that has friction, use equations (21) and (22).</li> </ol> <p>Let's write those down in a dynamics function</p> PythonJulia <pre><code>mc = 0.2 # mass of the cart (kg)\nmp = 0.1 # mass of the pole (kg)\n\u2113 = 0.5 # distance to the center of mass (meters)\ng = 9.81\n# (1)\n\ndef cartpole_dynamics(x, u):\n    r       = x[0] # cart position\n    theta   = x[1] # pole angle\n    rd      = x[2] # change in cart position\n    theta_d = x[3] # change in pole angle\n    F       = u[0] # force applied to cart\n\n    theta_dd = (g*np.sin(theta) + np.cos(theta) * ((-F - mp*l*(theta_d**2) * \\\n                    np.sin(theta))/(mc + mp))) / (l*(4/3 - (mp*(np.cos(theta)**2))/(mc + mp)))\n    rdd = (F + mp*l*((theta_d**2)*np.sin(theta) - theta_dd*np.cos(theta))) / (mc + mp)\n\n    return np.array([rd, theta_d, rdd, theta_dd])\n</code></pre> <ol> <li>Good practice would be to add an argument to \\(\\text{cartpole_dynamics}\\) that stores each of these parameters.</li> </ol> <pre><code>mc = 0.2 # mass of the cart (kg)\nmp = 0.1 # mass of the pole (kg)\n\u2113 = 0.5 # distance to the center of mass (meters)\ng = 9.81\n# (1)\n\nfunction cartpole_dynamics(x::Vector, u::Vector)\n    r  = x[1] # cart position\n    \u03b8  = x[2] # pole angle\n    rd = x[3] # change in cart position\n    \u03b8d = x[4] # change in pole angle\n    F  = u[1] # force applied to cart\n\n    \u03b8dd = (g*sin(\u03b8) + cos(\u03b8) * ((-F - mp*\u2113*(\u03b8d^2) * sin(\u03b8))/(mc + mp))) /\n             (\u2113*(4/3 - (mp*(cos(\u03b8)^2))/(mc + mp)))\n    rdd = (F + mp*\u2113*((\u03b8d^2)*sin(\u03b8) - \u03b8dd*cos(\u03b8))) / (mc + mp)\n\n    return [rd; \u03b8d; rdd; \u03b8dd]\nend\n</code></pre> <ol> <li>Good practice would be to add an argument to \\(\\text{cartpole_dynamics}\\) that stores each of these parameters.</li> </ol> <p>This function describes the continuous (nonlinear) dynamics of our system, i.e. \\(\\dot{x} = \\text{cartpole_dynamics}(x, u)\\). Before linearizing, we first discretize our continuous dynamics with an integrator. RK4 (Runge-Kutta 4th order) is a common explicit integrator, but you can write down whatever you like.</p> PythonJulia <pre><code>def cartpole_rk4(x, u, dt):\n    f1 = dt*cartpole_dynamics(x, u)\n    f2 = dt*cartpole_dynamics(x + f1/2, u)\n    f3 = dt*cartpole_dynamics(x + f2/2, u)\n    f4 = dt*cartpole_dynamics(x + f3, u)\n    return x + (1/6)*(f1 + 2*f2 + 2*f3 + f4)\n</code></pre> <pre><code>function cartpole_rk4(x::Vector, u::Vector, dt::Float64)\n    f1 = dt*cartpole_dynamics(x, u)\n    f2 = dt*cartpole_dynamics(x + f1/2, u)\n    f3 = dt*cartpole_dynamics(x + f2/2, u)\n    f4 = dt*cartpole_dynamics(x + f3, u)\n    return x + (1/6)*(f1 + 2*f2 + 2*f3 + f4)\nend\n</code></pre> <p>Our integrator takes in the state and control at the current time step and integrates the state forward by \\(\\Delta t\\) seconds (the dt parameter in the RK4 function). We now have the discrete (nonlinear) dynamics of our system, defined by \\(x_{k+1} = \\text{cartpole_rk4}(x_k, u_k, dt)\\). Now, we linearize about an equilibrium position to obtain state-transition and input matrices \\(A\\) and \\(B\\) we described earlier. Differentiating \\(\\text{cartpole_rk4}\\) by hand would be a pain, but luckily we have access to automatic differentiation tools to do this for us.</p> PythonJulia <pre><code>import autograd as AG\n\nxgoal = np.array([0.0, np.pi, 0.0, 0.0])\nugoal = np.array([0.0])\n\ndt = 0.01\n\nA = AG.jacobian(lambda x_: cartpole_rk4(x_, ugoal))(xgoal)\nB = AG.jacobian(lambda u_: cartpole_rk4(xgoal, u_))(ugoal)\n</code></pre> <pre><code>import ForwardDiff as FD\n\nxgoal = [0; pi; 0; 0]\nugoal = [0]\n\ndt = 0.01\n\nA = FD.jacobian(dx -&gt; cartpole_rk4(params, dx, ugoal, dt), xgoal)\nB = FD.jacobian(du -&gt; cartpole_rk4(params, xgoal, du, dt), ugoal)\n</code></pre> <p>Now all you have to do is save \\(A\\) and \\(B\\) and pass them to TinyMPC as shown in the problem setup section of the examples page.</p>"},{"location":"solver/background/","title":"Background","text":""},{"location":"solver/background/#overview","title":"Overview","text":"<p>The underlying algorithm is the alternating direction method of multipliers. TinyMPC reformulates the primal update step - the part that usually takes the longest - as an LQR problem. These have been studied for decades, and we know how to write LQR problems in a closed form: specifically, using Riccati recursion. We reorganize some of this recursive function to extract big matrices that only need to be computed once. In the vanilla implementation, this restricts TinyMPC to solving only a linear trajectory tracking problem (with any kinds of constraints, as long as they can be quickly re-linearized online). However, as seen in our demo videos, a single linearization can go a long way.</p>"},{"location":"solver/background/#alternating-direction-method-of-multipliers-admm","title":"Alternating direction method of multipliers (ADMM)","text":"<p>The alternating direction method of multipliers algorithm was developed in the 1970s and used in 2011 by researchers at Stanford to better solve the problem of distributed convex optimization. Some of these researchers later helped in developing OSQP, the Operator Splitting Quadratic Program solver. TinyMPC takes much of its inspiration from these two sources.</p> <p>We want to solve optimization problems in which our cost function \\(f\\) and set of valid states \\(\\mathcal{C}\\) are both convex:</p> \\[ \\begin{alignat}{2} \\min_x &amp; \\quad f(x) \\\\ \\text{subject to} &amp; \\quad x \\in \\mathcal{C}. \\end{alignat} \\] <p>We define an indicator function for the set \\(\\mathcal{C}\\):</p> \\[ I_\\mathcal{C}(x) = \\begin{cases} 0 &amp; x \\in \\mathcal{C} \\\\ \\infty &amp; \\text{otherwise}. \\end{cases} \\] <p>The indicator function says simply that there is infinite additional cost when \\(x\\) violates the constraints (the state \\(x\\) is outside the set of valid states \\(\\mathcal{C}\\)) and zero additional cost for obeying the constraints (\\(x\\) is inside the set \\(\\mathcal{C}\\)). Thus, we need to be able to determine whether or not a state is in the set \\(\\mathcal{C}\\) in order to know if all the constraints on our problem are being met. For speed of computation, this often takes the form \\(Hx \\geq h\\) (or \\(Hx \\leq h\\), or a combination of \\(Hx \\geq h\\) and \\(Gx \\leq g\\) (each of these can be rewritten to be equivalent to the others)). This form can describe any kind of linear constraint in \\(x\\). To do obstacle avoidance, for example, it is common to arrange \\(H\\) and \\(h\\) as half-space constraints where, in three dimensions, the entire space is split by a plane and only one half is inside the set \\(\\mathcal{C}\\). For arbitrary dimensionality, we say the space is divided by a hyperplane.</p> <p>We modify the generic optimization problem to include the indicator function by adding it to the cost. We introduce a new state variable \\(z\\), called the slack variable, to describe the constrained version of the original state variable \\(x\\), which we will now call the primal variable.</p> \\[ \\begin{alignat}{2} \\min_x &amp; \\quad f(x) + I_\\mathcal{C}(z) \\\\ \\text{subject to} &amp; \\quad x = z. \\end{alignat} \\] <p>At minimum cost, the primal variable \\(x\\) must be equal to the slack variable \\(z\\), but during each solve they will not necessarily be equal. This is because the slack variable \\(z\\) manifests in the algorithm as the version of the primal variable \\(x\\) that has been projected onto the feasible set \\(\\mathcal{C}\\), and thus whenever the primal variable \\(x\\) violates any constraint, the slack variable at that iteration will be projected back onto \\(\\mathcal{C}\\) and thus differ from \\(x\\). To push the primal variable \\(x\\) back to the feasible set \\(\\mathcal{C}\\), we introduce a third variable, \\(\\lambda\\), called the dual variable. This method is referred to as the augmented Lagrangian (originally named the method of multipliers), and introduces a scalar penalty parameter \\(\\rho\\) alongside the dual variable \\(\\lambda\\) (also known as a Lagrange multiplier). The penalty parameter \\(\\rho\\) is the augmentation to what would otherwise just be the Lagrangian of our constrained optimization problem above. \\(\\lambda\\) and \\(\\rho\\) work together to force \\(x\\) closer to \\(z\\) by increasing the cost of the augmented Lagrangian the more \\(x\\) and \\(z\\) differ.</p> \\[ \\mathcal{L}_A(x,z,\\lambda) = f(x) + I_\\mathcal{C}(z) + \\lambda^\\intercal(x-z) + \\frac{\\rho}{2}\\|x-z\\|^2_2. \\] <p>Our optimization problem has now been divided into two variables: the primal \\(x\\) and slack \\(z\\), and we can optimize over each one individually while holding all of the other variables constant. To get the ADMM algorithm, all we have to do is alternate between solving for the \\(x\\) and then for the \\(z\\) that minimizes our augmented Lagrangian. After each set of solves, we then update our dual variable \\(\\lambda\\) based on how much \\(x\\) differs from \\(z\\).</p> \\[ \\begin{alignat}{3} \\text{primal update: } &amp; x^+ &amp; ={} &amp; \\underset{x}{\\arg \\min} \\hspace{2pt} \\mathcal{L}_A(x,z,\\lambda), \\\\ \\text{slack update: } &amp; z^+ &amp; ={} &amp; \\underset{z}{\\arg \\min} \\hspace{2pt} \\mathcal{L}_A(x^+,z,\\lambda), \\\\ \\text{dual update: } &amp; \\lambda^+ &amp; ={} &amp; \\lambda + \\rho(x^+ - z^+), \\end{alignat} \\] <p>where \\(x^+\\), \\(z^+\\), and \\(\\lambda^+\\) refer to the primal, slack, and dual variables to be used in the next iteration.</p> <p>Now all we have to do is solve a few unconstrained optimization problems!</p>"},{"location":"solver/background/#todo-primal-and-slack-update-and-discrete-algebraic-riccati-equation","title":"TODO: primal and slack update and discrete algebraic riccati equation","text":""},{"location":"solver/solver/","title":"Inside TinyMPC","text":"<p>Our 2024 ICRA submission video provides a concise overview of the solver:</p> <p>Watch the Video </p>"},{"location":"solver/solver/#problem-formulation","title":"Problem formulation","text":"<p>TinyMPC solves convex quadratic model-predictive control programs of the form</p> \\[ \\begin{array}{ll}   \\mbox{minimize} &amp; \\sum_{k=0}^{N-1} \\frac{1}{2} (x_k - \\bar{x}_k)^T Q (x_k - \\bar{x}_k) + \\frac{1}{2}(u_k - \\bar{u}_k)^T R (u_k - \\bar{u}_k) \\\\   \\mbox{subject to} &amp; x_{0} = x_{\\text{init}},  \\\\                     &amp; x_{k+1} = A x_k + B u_k, \\\\                     &amp; u_k^l \\le u_k \\le u_k^u, \\\\                     &amp; x_k^l \\le x_k \\le x_k^u, \\\\                     &amp; u_k \\in \\mathcal{K}_u, \\\\                     &amp; x_k \\in \\mathcal{K}_x \\end{array} \\] <p>where \\(x_k \\in \\mathbb{R}^n\\), \\(u_k \\in \\mathbb{R}^m\\) are the state and control input at time step \\(k\\), \\(N\\) is the number of time steps (also referred to as the horizon), \\(A \\in \\mathbb{R}^{n \\times n}\\) and \\(B \\in \\mathbb{R}^{n \\times m}\\) define the system dynamics, \\(Q \\succeq 0\\), \\(R \\succ 0\\), and \\(Q_f \\succeq 0\\) are symmetric cost weight matrices and \\(\\bar{x}_k\\) and \\(\\bar{u}_k\\) are state and input reference trajectories. Constrains include state and input lower and upper bounds and second-order cones \\(\\mathcal{K}\\).</p>"},{"location":"solver/solver/#algorithm","title":"Algorithm","text":"<p>Will be provided soon! Meanwhile, check out our papers or background for more details.</p>"},{"location":"solver/solver/#implementations","title":"Implementations","text":"<p>The TinyMPC library offers a C++ implementation of the algorithm mentioned above, along with interfaces to several high-level languages. This integration allows these languages to seamlessly solve optimal control problems using TinyMPC.</p> <p>There are also several community-developed implementations of this algorithm: Rust</p> <p>Numerical benchmarks against other solvers on microcontrollers are available at this repository.</p> <p>Crazyflie firmware with TinyMPC is available at this repository.</p>"}]}